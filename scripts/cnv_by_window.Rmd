---
title: "CNV calling over windows"
author: "Simon Renny-Byfield"
date: "July 1, 2015"
output: html_document
---

The purpose of the document is to detail the analysis of CNV over windows across the genome. In this case the approach is similar to that detailed in `custom_cnv_v2.Rmd` in that it uses a z-score (e.g. standard deviations away from the mean) and the difference in read-depth between the sample and a reference genome (in this case B73). As such, it borrows heavily from the code employed in `custom_cnv_v2.Rmd`, with some slight modificartions.

##Generating the coverage data

The coverage information is calcualted using `.bam` files generated by Vince Buffalo and the following script.

```
#!/bin/bash -l
#!/bin/bash
#SBATCH -D /group/jrigrp4/CNVsim/data/windows
#SBATCH -o /group/jrigrp4/custom_cnv/logs/mcov_out_log-%j.txt
#SBATCH -e /group/jrigrp4/custom_cnv/logs/mcov_err_log-%j.txt
#SBATCH -J SimMulticov
#SBATCH --array=0-4
#SBATCH --mem-per-cpu=12000
#SBATCH --cpus-per-task=1

##Simon Renny-Byfield, UC Davis, June 2015

# define the windows of interest
windows=(50 250 500 1000 2000)

echo "Job Starting: "
date
echo "Working on window size ${windows[$SLURM_ARRAY_TASK_ID]}"

cmd="bedtools multicov -bams /group/jrigrp4/CNVsim/data/windows/*sorted.bam -bed <(bedtools makewindows -g ../../../custom_cnv/data/chrLenFilev2.txt -w ${windows[$SLURM_ARRAY_TASK_ID]}) -q 30 > window_${windows[$SLURM_ARRAY_TASK_ID]}bp_coverage.txt"
echo $cmd
eval $cmd

echo "Jobe Done: "
date
``` 

Next, the data have to be GC normalized using a script that looks like this:

```
#!/bin/bash -l
#SBATCH -D /group/jrigrp4/custom_cnv/data
#SBATCH -o /group/jrigrp4/custom_cnv/logs/gc_out_log-%j.txt
#SBATCH -e /group/jrigrp4/custom_cnv/logs/gc_err_log-%j.txt
#SBATCH -J gc
#SBATCH --array=0-2
#SBATCH --mem-per-cpu=20000
#SBATCH --cpus-per-task=1

##Simon Renny-Byfield, UC Davis, April 2015
 

windows=(500 1000 2000)

echo "Starting Job:"
date

cmd="Rscript ../scripts/gcNorm_sim.R window_${windows[$SLURM_ARRAY_TASK_ID]}bp_coverage.txt window_${windows[$SLURM_ARRAY_TASK_ID]}bp_GC.txt gc_normalized_${windows[$SLURM_ARRAY_TASK_ID]}bp_median.RData"
echo $cmd
eval $cmd

echo "Job done: "
date
```

This scripts calls an `R` script which utilizes the `EDASeq` package and looks like this:

```{r,eval=FALSE}
###
# Normalize read-depth over sliding windows across the Maize genome
###

# Simon Renny-Byfield, June '15, UC Davis

options(echo=TRUE) # if you want see commands in output file
args <- commandArgs(trailingOnly = TRUE)
print(args)

# load some useful libraries
library(data.table)
library(ggplot2)
library(reshape2)
library(EDASeq)

# load in the data
chr6cov<-read.table(args[1])

norm.df<-subset(chr6cov, select=-c(1:3))
# normalize by column total
norm.df<-sweep(norm.df, 2, colSums(subset(norm.df)), FUN="/")
norm.df<-sweep(norm.df, 2, 1e6, FUN="*")

#load in teh gc data
ch6_gc<-read.table(args[2])
#attach it to norm.df

gc<-ch6_gc$V5
norm.df<-data.matrix(norm.df)
# now try to normalize the data according to GC content
chr6Coverage<-withinLaneNormalization(x=norm.df, y=gc, which="median", round=FALSE)
# modify make a new data.table of the gc normalized counts
chr6Coverage<-data.table(chr6Coverage)
chr6Coverage<-cbind(chr6cov[,1:3],chr6Coverage,gc)

save(chr6Coverage,file=args[3])
```

##Generating the Tajima's D over the same windows

Next we need to generate the Tajima's D data. This is done using `ANGSD` as follows:

```
#!/bin/bash -l
#!/bin/bash
#SBATCH -D /group/jrigrp4/custom_cnv/sfs/windows
#SBATCH -o /group/jrigrp4/custom_cnv/logs/sfs_out_log-%j.txt
#SBATCH -e /group/jrigrp4/custom_cnv/logs/sfs_err_log-%j.txt
#SBATCH -J SFS&TD
#SBATCH --mem-per-cpu=10000
#SBATCH --cpus-per-task=18

##Simon Renny-Byfield, UC Davis, June 2015

echo "Starting Job:"
date

# calculate the .saf file
cmd="angsd -bam ../../../teosinte_parents/file.list.txt -doSaf 2 -out teosinte19 -anc ../../../teosinte_parents/genomes/TRIP.fa -ref ../../../teosinte_parents/genomes/Zea_mays.AGPv3.22.dna.genome.fa -GL 1 -P 18 -indF ../../../teosinte_parents/angsd_outpu$
echo $cmd
#eval $cmd

# formally calculate the SFS
cmd="realSFS teosinte19.saf 38 -maxIter 100 -P 12 > teosinte19.sfs"
echo $cmd
#eval $cmd

# no fiure out region wide thetas
cmd="angsd -bam ../../../teosinte_parents/file.list.txt -out teosinte19_thetas.sfs -doThetas 1 -doSaf 2 -ref ../../../teosinte_parents/genomes/Zea_mays.AGPv3.22.dna.genome.fa -indF ../../../teosinte_parents/angsd_output/teo_parents19e-6.indF -pest teosin$
echo $cmd
#eval $cmd

# make the .bed file
cmd="thetaStat make_bed teosinte19_thetas.sfs.thetas.gz"
echo $cmd
eval $cmd

#calculate Tajimas D
cmd="thetaStat do_stat teosinte19_thetas.sfs.thetas.gz -nChr 19 -win 1000 -step 1000 -outnames teosinte19.teothetasWindow.gz"
echo $cmd
eval $cmd

#now clean up the output..
cmd="cat teosinte19.teothetasWindow.gz.pestPG | awk '$14!="0" {print}' > teosinte19.TajD.txt"
echo $cmd
#eval $cmd

echo "End Job: "
date
```

Do calculate Tajima'D window of varying sizes we can use a batch script that look something like this:

```
#!/bin/bash -l
#!/bin/bash
#SBATCH -D /group/jrigrp4/custom_cnv/sfs/windows
#SBATCH -o /group/jrigrp4/custom_cnv/logs/sfs_out_log-%j.txt
#SBATCH -e /group/jrigrp4/custom_cnv/logs/sfs_err_log-%j.txt
#SBATCH -J SFS&TD
#SBATCH --array=0-4
#SBATCH --mem-per-cpu=5000
#SBATCH --cpus-per-task=18

##Simon Renny-Byfield, UC Davis, June 2015

windows=(2000 1000 500 250 50)

winSize=${windows[$SLURM_ARRAY_TASK_ID]}

echo "Starting Job:"
date

#calculate Tajimas D
cmd="thetaStat do_stat teosinte19_thetas.sfs.thetas.gz -nChr 19 -win $winSize -step $winSize -outnames teosinte19.teothetasWindow$winSize.gz"
echo $cmd
eval $cmd

echo "Ending Job: "
date
```

The data then need to be modifed using a custom perl script `angsd2GenomicRanges.pl` that converts the output of `ANGSD` into something GenomicRanges can understand. 

```
#!/usr/bin/perl
use strict;
use warnings;

# Simon Renny-Byfield, UC Davis, March 17th 2015

#usage script.pl <angsd output> >outfile

# This script will take angsd output (.pestPG file) and convert format suitable 
# for input into GenomicRanges. The purpose is to associate regions in maize genome
# that have estimates of Tajima's D and relate those estimates to particular genes. 
# In short each window from the angsd analysis will be labelled with the corresponding 
# gene name using GenomicRanges. 

# The output from angsd looks like this..

# ## thetaStat VERSION: 0.01 build:(Oct 23 2014,15:39:05)
# #(indexStart,indexStop)(firstPos_withData,lastPos_withData)(WinStart,WinStop)  Chr	WinCenter	tW	tP	tF	tH	tL	Tajima	fuf	fud	fayh	zeng	nSites
# (25,53)(1778322,1778350)(1778250,1778350)	1	1778300	1.661604	0.550991	3.218805	1.968841	1.259916	-2.122087	-1.758895	-1.220118	-0.736941	-0.208055	28
# (25,96)(1778322,1778532)(1778300,1778400)	1	1778350	3.663915	2.025260	7.192596	3.483926	2.754594	-1.622298	-1.918688	-1.580332	-0.370203	-0.240505	71
# (53,96)(1778350,1778532)(1778350,1778450)	1	1778400	2.002310	1.474270	3.973791	1.515086	1.494677	-0.869529	-1.449146	-1.362891	-0.017994	-0.225741	43
# (96,127)(1778532,1778731)(1778500,1778600)	1	1778550	0.235098	0.097436	0.000001	0.005128	0.051282	-0.980203	0.205032	0.575739	0.213366	-0.367884	31
# (114,127)(1778550,1778731)(1778550,1778650)	1	1778600	0.235098	0.097436	0.000001	0.005128	0.051282	-0.980203	0.205032	0.575739	0.213366	-0.367884	13
# (127,146)(1778731,1778750)(1778650,1778750)	1	1778700	0.000504	0.000107	0.002142	0.000003	0.000055	-0.066506	-0.096080	-0.089562	0.006211	-0.021466	19

#but a suitable input into GenomicRanges might look like this

# ## thetaStat VERSION: 0.01 build:(Oct 23 2014,15:39:05)
# #(indexStart,indexStop)(firstPos_withData,lastPos_withData)(WinStart,WinStop)	Chr	WinCenter	tW	tP	tF	tH	tL	Tajima	fuf	fud	fayh	zeng	nSites
# 1 1778250	1778350	1778300	1.661604	0.550991	3.218805	1.968841	1.259916	-2.122087	-1.758895	-1.220118	-0.736941	-0.208055	28

#where the first three columns are chr, start, stop.

####
# Read in the data
####

# open the input file

open ( IN , "<$ARGV[0]" ) || die "could not open file:$!\n";

####
# Cycle through the file modifying each line
####

while ( <IN> )  {
	if ( m/##/ ) {
		print;
		next
	}
	if ( m/#\(/ ) {
		print "chr	start	stop	WinCenter	tW	tP	tF	tH	tL	Tajima	fuf	fud	fayh	zeng	nSites\n";
		next
	}#if
	chomp;
	my @data = split "\t";
	# grab the start and stop of each window
	$data[0] =~ m/\(\d+,\d+\)\(\d+,\d+\)\((\d+,\d+)\)/;
	my $new = $1;
	#replace a comma with tab
	$new =~ s/,/\t/;
	#print the new line as you want it.
	print $data[1] , "\t" , $new , "\t" , join( "\t" , @data[2 .. $#data]) , "\n";
}#while

exit;


```

## Unifiying the coverage and Tajima'D datasets

Unfortunately not all windows are present in the resulting output (I think this is because not all windows have data) and so we need an additional few steps to make sure we keep the windows in order. To do this we can take advantge of the `GenomicRanges` package in R.

First load in some libraries, the list of these can be expanded as we need them.

```{r,warning=FALSE,message=FALSE}
library(data.table)
library(GenomicRanges)
library(rtracklayer)
library(scales)

# now set the working dir
setwd("/Users/simonrenny-byfield/GitHubRepos/cnvwin")
```

Then we can unify the data.

```{r,warning=FALSE, message=FALSE}

# load in the TD data
td.data<-fread("/Users/simonrenny-byfield/GitHubRepos/cnvwin/data/TD_windows_2000.txt")
# turn into a GRanges object
td.ranges<-GRanges(seqnames=td.data$chr, ranges=IRanges(td.data$start,td.data$stop),td=td.data$Tajima)

# load in the coverage data
load("/Users/simonrenny-byfield/GitHubRepos/cnvwin/data/gc_normalized_2000bp_median.RData")
gcNorm<-chr6Coverage
# turn into a GRanges object
gcNorm<-GRanges(seqnames=gcNorm$V1, ranges=IRanges(gcNorm$V2,gcNorm$V3))

###
# Check for overlaps and unify the data
###

# first make a variable not modify, with NA values
values(gcNorm)$td<-NA
# now check for overlaps, require "equal" because we want complete overlap
overlaps<-as.matrix(findOverlaps(gcNorm,td.ranges, type="equal"))
# use the hit matrix to add a value of TD to the coverage data set
values(gcNorm)$td[overlaps[,1]]<-td.ranges$td[overlaps[,2]]

# now turn in back into a datatable
gcNorm<-data.table(chr=as.vector(seqnames(gcNorm)),start=start(gcNorm),end=end(gcNorm),td=gcNorm$td)
gcNorm<-cbind(gcNorm,chr6Coverage$V4,chr6Coverage$V5)

# do some clearing up
chr6Coverage<-NULL
gc()
```

## Load in the gene annotation data

Now we have unified the coverage and Tajima's D datasets we can now begin to load in the gene annotation data. Included with this will be the neccesary maize1 and maize2 data.

```{r}
# load in the annotation data
gff<-fread("/Users/simonrenny-byfield/GitHubRepos/cnvwin/data/AGPv3.22_primary_transcripts.bed")
gff<-GRanges(seqnames=gff$V1, ranges=IRanges(gff$V2,gff$V3),name=gff$V4)
# remove scaffolds
gff<-subset(gff,subset=seqnames %in% c(1:10))

# prepare the GRanges object to receive the sub-genome info
values(gff)$sub.genome<-"no syn"
# load in the maize1 maize 2 subgenomes info
subGenomes<-read.csv("/Users/simonrenny-byfield/GitHubRepos/custom_cnv/data/gene_by_sugenome.csv",na.strings=c("",".","NA"))
# attribute the correct sub-genome to each annotation
values(gff)$sub.genome[gff$name %in% subGenomes[,1]]<-"M1"
values(gff)$sub.genome[gff$name %in% subGenomes[,2]]<-"M2"

```

## Add samples names to the dataset

```{r,warning=FALSE,message=FALSE}
samples<-c("B73","test")
setnames(gcNorm,colnames(gcNorm),c(colnames(gcNorm)[1:4],samples))
head(gcNorm)
```

## Filtering the data

Before we start, it might be a good idea to screen out windows that have really high coverage in B73, these are likely to represent repeatitive or poorly assembled region of the genome and should be excluded from further consideration.

```{r,warning=FALSE,message=FALSE}
#set the max read depth
maxDepth<-15
minDepth<-1
gcNorm<-subset(gcNorm,subset=gcNorm$B73 < maxDepth )
gcNorm<-subset(gcNorm, subset= gcNorm$B73 > minDepth)

```

## Runnig the CNV calling function

```{r, message=FALSE,warning=FALSE}
source("/Users/simonrenny-byfield/GitHubRepos/cnvwin/scripts/callCNV.R")
test<-callCNV(gcNorm,samples,limit=1,limitHom=2)
dev.off()
# take a look at the genic region in terms of coverage
    cov<-GRanges(seqnames=gcNorm$chr, ranges=IRanges(gcNorm$start,gcNorm$end),coverage=subset(gcNorm,select="B73"))
    # check for overlaps between gcNorm and gff
    geneData<-subsetByOverlaps(cov,gff,type = "within")
```


Next, the best thing to do it to calculate the allele frequencies based on the genotype calls. We can do this using an apply function over the `gcNorm` table, using `sum` as a function. Later we can trim the CNVs we want by looking at only those with exclusively up, or down calls etc. At this stage:

* 0 is equivalent to homozygous for a deletion.
* 1 is equivalent heterozygous for a deletion.
* 2 is normal.
* 3 is more than two copies of a region.
